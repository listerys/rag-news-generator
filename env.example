# Congress.gov API Configuration
# Get your API key from: https://api.congress.gov/sign-up/
CONGRESS_API_KEY=your_api_key_here

# Ollama Configuration (Local LLM inference)
# The Ollama service runs locally in Docker with GPU support
# No API key needed - fully open source and local
OLLAMA_URL=http://ollama:11434

# LLM Model Selection:
# - llama3.1:8b (4.7GB, high quality, ~11 min on CPU)
# - phi3:medium (2.3GB, good quality, ~8-9 min on CPU) [RECOMMENDED FOR CPU]
# - mistral:7b-instruct (4.1GB, good alternative, ~10 min on CPU)
LLM_MODEL=llama3.1:8b

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=kafka:29092

# Redis Configuration
REDIS_URL=redis://redis:6379

# PostgreSQL Configuration
DATABASE_URL=postgresql://raguser:ragpass@postgres:5432/ragdb

# Optional: Performance tuning
# WORKER_REPLICAS=8
